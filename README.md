# webscraping-examples
## Passo a passo de como criar o seu primeiro scraper do zero!
### Objetivo: 
Ensinar a desenvolver um scraper do zero do [site da Nike](https://www.nike.com.br/nav/categorias/tenis/genero/feminino/tipodeproduto/calcados), visando extrair as informa√ß√µes dos t√≠tulos e pre√ßos dos cal√ßados femininos.
### Requisitos de instala√ß√£o:
- [Python](https://www.python.org/downloads/);
- Biblioteca [Requests](https://pypi.org/project/requests/);
- Biblioteca [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#) OU biblioteca [LXML](https://lxml.de/installation.html).
### Sugest√µes:
- [Postman](https://www.postman.com/) OU [Insomnia](https://insomnia.rest/) instalado, ou alguma ferramenta para testes de requisi√ß√µes;
- [SelectorGadget](https://selectorgadget.com/).

#### PASSO A PASSO:
**1.** Definir o site que se deseja realizar o scraping e quais as informa√ß√µes voc√™ deseja extrair dele;

**2.** Acessar o site pelo pr√≥prio navegador e analisar em qual formato √© retornado o conte√∫do da p√°gina (JSON, HTML/XML‚Ä¶);

Dica: para analisar isso voc√™ pode utilizar o atalho Ctrl+Shift+i ou clicar com o bot√£o direito do mouse na p√°gina e clicar em Inspecionar. Ir√° abrir o dev tools, o famoso ferramentas do desenvolvedor. Ali voc√™ pode analisar tudo sobre a requisi√ß√£o! Segue um link para apoio caso tenha alguma dificuldade: [Como abrir o Dev Tools](https://support.google.com/adsense/answer/10858959?hl=pt-BR#:~:text=Para%20abrir%20o%20DevTools%2C%20clique,%2C%20Linux%2C%20Chrome%20OS).

Depois voc√™ deve selecionar qual a requisi√ß√£o que lhe interessa o conte√∫do. No meu caso √© a que tem o nome de calcados, pois foi para esse endpoint que eu efetuei a requisi√ß√£o. Primeiramente me certifico de estar na aba de Network, ent√£o clico em calcados (1) e depois na aba de Response (2), para poder analisar o conte√∫do da resposta da requisi√ß√£o. Neste caso, o conte√∫do foi retornado em formato HTML:
<p align="center">
  <img src="/images/check_response.png" width="700">
</p>

**3.** Fa√ßa um teste de requisi√ß√£o utilizando uma aplica√ß√£o de testes de API, como o Postman ou o Insomnia;

Dica: Aqui voc√™ pode utilizar a ferramenta CurlConverter para te fornecer todas as informa√ß√µes necess√°rias para a requisi√ß√£o (headers, payload, url‚Ä¶). Deixo aqui um [Tutorial sobre Curl](https://www.hostinger.com.br/tutoriais/comando-curl-linux), caso desconhe√ßa o conceito.
Ao clicar com o bot√£o direito em cima da sua requisi√ß√£o > Copy > Copy as curl (bash), voc√™ ter√° o conte√∫do da sua requisi√ß√£o em curl:
<p align="center">
  <img src="/images/copy_curl.png" width="700">
</p>

Depois √© s√≥ acessar uma ferramenta de conversor de curl, como o [CurlConverter](https://curlconverter.com/), e colar o conte√∫do curl da requisi√ß√£o (1):

<p align="center">
  <img src="/images/curlconverter.png" width="700">
</p>
A ferramenta ir√° ent√£o fornecer j√° o c√≥digo para o scraper conforme a linguagem de sua prefer√™ncia (2), junto com todas as informa√ß√µes necess√°rias para a requisi√ß√£o:
<p align="center">
  <img src="/images/curlconverter2.png" width="700">
</p>
OBS: A ferramenta traz como padr√£o headers e cookies para todas as requisi√ß√µes. Contudo, voc√™ vai perceber que nem sempre √© necess√°rio informar headers customizados para realizar uma requisi√ß√£o, e os cookies s√£o sempre din√¢micos, ent√£o n√£o devem ser utilizados de forma est√°tica.

Depois voc√™ pode utilizar o Postman para testar a sua requisi√ß√£o:
<p align="center">
  <img src="/images/postman_request.png" width="700">
</p>

O Postman √© uma ferramenta de testes de APIs muito rica. N√£o irei entrar em detalhes, mas deixarei um [Tutorial de uso do Postman](https://learning.postman.com/docs/introduction/overview/) para voc√™ consultar caso surjam d√∫vidas ao us√°-lo.

Continuando... Para o caso do site da Nike, por exemplo, n√£o foi necess√°rio colocar headers customizados. Somente com a URL a requisi√ß√£o j√° foi realizada com sucesso (c√≥digo de status 200 e retorno conforme esperado!). Se voc√™ observar, na aba de Headers aparecem 7 par√¢metros, mas eu n√£o cadastrei nenhum ü§î. Isso ocorre porque o pr√≥prio Postman j√° envia alguns par√¢metros de header como padr√£o:
<p align="center">
  <img src="/images/postman_headers_default.png" width="700">
</p>

OBS: Caso voc√™ simule a conex√£o e o c√≥digo de status da requisi√ß√£o n√£o seja o esperado, √© poss√≠vel que seja necess√°rio adicionar novos par√¢metros nos headers ou alterar os par√¢metros padr√£o do Postman.


**4.** M√£o no c√≥digo! Agora √© hora de construir o seu c√≥digo (ou copiar do CurlConverter) e test√°-lo.

Abaixo temos a primeira vers√£o do c√≥digo do nosso scraper, fornecido pelo pr√≥prio CurlConverter. O c√≥digo √© bem simples, apenas importamos a biblioteca requests (respons√°vel pelas requisi√ß√µes), definimos os headers e chamamos o m√©todo GET para realizar a requisi√ß√£o. Passamos como argumento tanto a URL quanto os headers.
OBS: √© necess√°rio conhecer quais s√£o os m√©todos de uma requisi√ß√£o HTTP, caso lhe interessar: [M√©todos HTTP](https://www.escoladnc.com.br/blog/aprenda-a-utilizar-os-metodos-http-get-post-e-put-em-python/)

```python
import requests

headers = {
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
    'accept-language': 'pt-PT,pt;q=0.9,en-US;q=0.8,en;q=0.7',
    'cache-control': 'max-age=0',
    'referer': 'https://www.nike.com.br/?gad_source=1&gclid=Cj0KCQjw8pKxBhD_ARIsAPrG45lu19se-z4tPHw_pGBQMeKBwcaN0JvmTj8mpNTK0mVFMQSe3Qf-BPsaAtZWEALw_wcB',
    'sec-ch-ua': '"Google Chrome";v="123", "Not:A-Brand";v="8", "Chromium";v="123"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    'sec-fetch-dest': 'document',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-user': '?1',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

text = requests.get(url="https://www.nike.com.br/nav/categorias/tenis/genero/feminino/tipodeproduto/calcados", headers=headers)
print(text.status_code) # 200
```

Maravilha! C√≥digo de status com retorno 200! OBS: √© importante entender o que cada c√≥digo de status de uma requisi√ß√£o HTTP representa. Neste [link](https://developer.mozilla.org/pt-BR/docs/Web/HTTP/Status) est√° bem completa a explica√ß√£o.

Mas saca s√≥, com o tempo voc√™ vai entendendo o que significa cada par√¢metro do header, e percebe que alguns s√£o mais importantes que outros!

```python
import requests

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

text = requests.get(url="https://www.nike.com.br/nav/categorias/tenis/genero/feminino/tipodeproduto/calcados", headers=headers)
print(text.status_code) # 200
```

Se eu removo todos os meus headers, com exce√ß√£o do user-agent, eu continuo tendo acesso a minha p√°gina normalmente.
Mas, se eu remover o user-agent tamb√©m, eu recebo um bloqueio no acesso √† p√°gina (c√≥digo 403):

```python
import requests

text = requests.get(url="https://www.nike.com.br/nav/categorias/tenis/genero/feminino/tipodeproduto/calcados")
print(text.status_code) # 403
```

Com isso, voc√™ entende que o √∫nico header necess√°rio (neste caso!) √© o user-agent, e que os demais s√£o dispens√°veis. E pronto, temos acesso ao conte√∫do da nossa p√°gina! Como guardamos o conte√∫do da requisi√ß√£o no objeto response, essa response possui alguns atributos, como o status_code (c√≥digo de resposta), content (conte√∫do/texto de retorno), url, headers, cookies‚Ä¶ lembrando que essas informa√ß√µes s√£o todas do objeto Response (resposta √† requisi√ß√£o). Para acessar essas informa√ß√µes, √© s√≥ digitar: response.status_code, response.text, e assim por diante‚Ä¶

**5**. PARSING -> Processamento do texto
J√° temos o retorno da p√°gina, por√©m, temos apenas o texto cru, sem separa√ß√£o de elementos. O que n√≥s queremos √© apenas os t√≠tulos e os valores dos produtos. Para isso, h√° bibliotecas dedicadas para o processamento de texto, como BeautifulSoup e Lxml. Neste tutorial vamos falar um pouco sobre as duas.

#### *- BeautifulSoup:*
√â uma biblioteca muito rica para lidar com p√°ginas HTML/XML. Super indico a leitura da biblioteca, pois aqui passarei apenas por cima das possibilidades. Pois bem, quando estudamos como a estrutura de um documento HTML funciona, entendemos que ele √© constru√≠do no formato de √°rvore. Assim, elementos possuem um relacionamento entre si: pai, primo, irm√£o. Conforme a imagem abaixo, onde, por exemplo, o elemento *div* √© pai do elemento *ul*, que √© pai dos elementos *li*. √â importante entender isso para conseguirmos captar os elementos desejados em um arquivo HTML.
<p align="center">
  <img src="/images/html_dom.png" width="500">
</p>

Ok, mas e como eu sei como capturar as tags espec√≠ficas que eu quero? Se voc√™ clicar na seta ali indicado no n√∫mero 1, e depois clicar no conte√∫do que voc√™ deseja pegar (2), ir√° aparecer tanto um popup com as informa√ß√µes do elemento, quanto na pr√≥pria resposta da requisi√ß√£o ir√° aparecer o elemento ao lado. Assim, voc√™ consegue ter acesso √†s informa√ß√µes do elemento: tipo da tag (se √© div, span, p‚Ä¶), texto, class, id‚Ä¶ essas informa√ß√µes s√£o essenciais para captarmos o conte√∫do do texto. Class e id s√£o formas de identificarmos cada elemento na p√°gina, seja de forma coletiva/em grupo (class) ou de forma individual (id). 

Para o nosso exemplo, segue o c√≥digo:
```python
import requests
from bs4 import BeautifulSoup

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

response = requests.get(url="https://www.nike.com.br/nav/categorias/tenis/genero/feminino/tipodeproduto/calcados", headers=headers)
soup = BeautifulSoup(response.content, "html.parser")
titles = soup.find_all(name="div", attrs={"class", "ProductCard-styled__ProductName-sc-8f840517-9 iKtbYK"})
prices = soup.find_all(name="span", attrs={"class", "ProductCard-styled__CurrentPrice-sc-8f840517-11 jyXUjS"})
```
Voc√™ primeiramente cria um objeto do BeautifulSoup, passando como conte√∫do a resposta da requisi√ß√£o, e informando qual o tipo de parser voc√™ deseja. Depois disso, a nossa vari√°vel soup j√° entende que aquele √© um arquivo html. Assim, voc√™ utilizar√° o m√©todo find_all para encontrar todos os elementos que tenham o respectivo tipo, e com os atributos informados acima. No caso do t√≠tulo dos cal√ßados, a informa√ß√£o est√° numa tag de tipo div, de classe *ProductCard-styled__ProductName-sc-8f840517-9 iKtbYK*. J√° os pre√ßos se encontram em tags do tipo span, de classe *ProductCard-styled__CurrentPrice-sc-8f840517-11 jyXUjS*.
Abaixo, na imagem √† esquerda, coloquei para printar na tela os produtos com seus respectivos valores. Conforme ilustrado na imagem √† direita, podemos verificar que alcan√ßamos o resultado esperado de acordo com o site:
<table>
  <tr>
    <td><img src="/images/comparison_terminal.png" alt="Descri√ß√£o da imagem 1"></td>
    <td><img src="/images/comparison_site.png" alt="Descri√ß√£o da imagem 2"></td>
  </tr>
</table>

#### *- LXML:*
Outra biblioteca que voc√™ pode utilizar √© o lxml, que nos possibilita trabalhar com express√µes em XPath. Junto com ele, uma extens√£o bacana √© o SelectorGadget, que nos fornece express√µes xpath de cada grupo de elementos. Ele √© uma extens√£o que voc√™ pode adicionar ao seu navegador e ser√° muito √∫til. Para utiliz√°-lo:
1. Voc√™ clica no canto superior direito no √≠cone da lupa;
2. Clique no elemento da p√°gina que voc√™ deseja capturar. A extens√£o busca padr√µes, ent√£o ao clicar em um elemento ele ficar√° verde e mostrar√° em amarelo todos os outros elementos que possuem semelhan√ßa. Caso voc√™ n√£o queira esses elementos em amarelo, √© s√≥ clicar em cima deles, que eles ser√£o desconsiderados. Como no meu caso os elementos em amarelo s√£o do meu interesse, deixo da forma como est√°;
3. Clique no bot√£o Xpath na parte inferior da tela;
4. Copie o Xpath fornecido no popup. Pronto! Agora voc√™ j√° tem o XPath, ou seja, a localiza√ß√£o do seu elemento XML.
<p align="center">
  <img src="/images/selector_elements.png" width="700">
</p>

Partindo para o c√≥digo, a l√≥gica √© parecida com a anterior, a diferen√ßa √© que agora usamos o XPath:
```python
import requests
from lxml import html

headers = {
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
}

response = requests.get(url="https://www.nike.com.br/nav/categorias/tenis/genero/feminino/tipodeproduto/calcados", headers=headers)
tree = html.fromstring(response.content)
titles = tree.xpath('//*[contains(concat( " ", @class, " " ), concat( " ", "iKtbYK", " " ))]')
prices = tree.xpath('//*[contains(concat( " ", @class, " " ), concat( " ", "jyXUjS", " " ))]')
```
Prontinho! Mesmo retorno obtido, apenas com uma tecnologia diferente! :)
